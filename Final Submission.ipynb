{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('Train.csv')\n",
    "test_df = pd.read_csv('Test.csv')\n",
    "\n",
    "def advanced_feature_engineering(df, is_training=True):\n",
    "    \"\"\"\n",
    "    Enhanced feature engineering with automotive domain expertise\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # 1. Core efficiency ratios\n",
    "    df_processed['engine_per_owner'] = df_processed['engine_capacity_cc'] / (df_processed['owner_count'] + 0.5)\n",
    "    df_processed['power_to_weight_ratio'] = df_processed['engine_capacity_cc'] / 1000  # Normalized power ratio\n",
    "    \n",
    "    # 2. Advanced fuel type efficiency mapping\n",
    "    fuel_efficiency_map = {\n",
    "        'Electric': 3.2, 'Hybrid': 2.1, 'Diesel': 1.45, \n",
    "        'Petrol': 1.0, 'CNG': 1.15, 'LPG': 1.1\n",
    "    }\n",
    "    df_processed['fuel_efficiency_factor'] = df_processed['fuel_category'].map(fuel_efficiency_map).fillna(1.0)\n",
    "    \n",
    "    # 3. Transmission efficiency with more categories\n",
    "    transmission_map = {\n",
    "        'Manual': 1.25, 'Automatic': 1.0, 'CVT': 1.1, \n",
    "        'Semi-Automatic': 1.15, 'AMT': 1.05\n",
    "    }\n",
    "    df_processed['transmission_efficiency'] = df_processed['transmission_type'].map(transmission_map).fillna(1.0)\n",
    "    \n",
    "    # 4. Enhanced brand efficiency scores\n",
    "    brand_efficiency = {\n",
    "        'Toyota': 1.3, 'Honda': 1.25, 'Hyundai': 1.2, 'Maruti': 1.25,\n",
    "        'Nissan': 1.15, 'Kia': 1.15, 'Volkswagen': 1.1, 'Ford': 1.05,\n",
    "        'Chevrolet': 0.95, 'BMW': 0.9, 'Mercedes': 0.85, 'Audi': 0.88,\n",
    "        'Tesla': 1.5, 'Tata': 1.1, 'Mahindra': 1.0\n",
    "    }\n",
    "    df_processed['brand_efficiency_score'] = df_processed['car_brand'].map(brand_efficiency).fillna(1.0)\n",
    "    \n",
    "    # 5. Sophisticated age depreciation\n",
    "    df_processed['age_efficiency_loss'] = np.exp(-0.08 * (df_processed['owner_count'] - 1))\n",
    "    df_processed['age_efficiency_loss'] = df_processed['age_efficiency_loss'].clip(lower=0.5)\n",
    "    \n",
    "    # 6. Engine displacement categories with efficiency curves\n",
    "    df_processed['engine_category'] = pd.cut(\n",
    "        df_processed['engine_capacity_cc'], \n",
    "        bins=[0, 800, 1200, 1600, 2000, 2500, 3000, 5000],\n",
    "        labels=[6, 5, 4, 3, 2, 1, 0]\n",
    "    ).astype(float)\n",
    "    \n",
    "    # 7. Multi-factor efficiency score\n",
    "    df_processed['combined_efficiency'] = (\n",
    "        df_processed['fuel_efficiency_factor'] * \n",
    "        df_processed['transmission_efficiency'] * \n",
    "        df_processed['brand_efficiency_score'] * \n",
    "        df_processed['age_efficiency_loss']\n",
    "    )\n",
    "    \n",
    "    # 8. Advanced engine metrics\n",
    "    df_processed['engine_load_factor'] = np.log1p(df_processed['engine_capacity_cc']) / 8\n",
    "    df_processed['displacement_per_owner'] = df_processed['engine_capacity_cc'] / (df_processed['owner_count'] ** 0.8)\n",
    "    \n",
    "    # 9. Market segment classification\n",
    "    luxury_brands = ['BMW', 'Mercedes', 'Audi', 'Tesla', 'Jaguar', 'Porsche']\n",
    "    economy_brands = ['Maruti', 'Tata', 'Hyundai', 'Honda', 'Toyota']\n",
    "    df_processed['is_luxury'] = df_processed['car_brand'].isin(luxury_brands).astype(int)\n",
    "    df_processed['is_economy'] = df_processed['car_brand'].isin(economy_brands).astype(int)\n",
    "    \n",
    "    # 10. Color efficiency (thermal properties)\n",
    "    color_efficiency = {\n",
    "        'White': 1.05, 'Silver': 1.03, 'Gray': 1.02, 'Blue': 1.01,\n",
    "        'Black': 0.97, 'Red': 0.98, 'Green': 1.0, 'Brown': 0.99\n",
    "    }\n",
    "    df_processed['color_efficiency'] = df_processed['exterior_color'].map(color_efficiency).fillna(1.0)\n",
    "    \n",
    "    # 11. Advanced interaction features\n",
    "    df_processed['fuel_engine_interaction'] = (\n",
    "        df_processed['fuel_efficiency_factor'] * \n",
    "        np.log1p(df_processed['engine_capacity_cc'])\n",
    "    )\n",
    "    df_processed['brand_age_interaction'] = (\n",
    "        df_processed['brand_efficiency_score'] * \n",
    "        df_processed['age_efficiency_loss']\n",
    "    )\n",
    "    df_processed['transmission_engine_interaction'] = (\n",
    "        df_processed['transmission_efficiency'] * \n",
    "        df_processed['engine_category']\n",
    "    )\n",
    "    \n",
    "    # 12. Polynomial features for key variables\n",
    "    df_processed['engine_squared'] = df_processed['engine_capacity_cc'] ** 2\n",
    "    df_processed['owner_squared'] = df_processed['owner_count'] ** 2\n",
    "    \n",
    "    # 13. Efficiency ratios\n",
    "    df_processed['efficiency_per_cc'] = df_processed['combined_efficiency'] / (df_processed['engine_capacity_cc'] + 1)\n",
    "    df_processed['brand_fuel_synergy'] = df_processed['brand_efficiency_score'] * df_processed['fuel_efficiency_factor']\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "def remove_outliers_comprehensive(df, target_col='fuel_efficiency_kmpl'):\n",
    "    \"\"\"\n",
    "    Multi-method outlier removal with statistical validation\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    initial_count = len(df_clean)\n",
    "    \n",
    "    # Method 1: Modified Z-score (more robust)\n",
    "    median = df_clean[target_col].median()\n",
    "    mad = np.median(np.abs(df_clean[target_col] - median))\n",
    "    modified_z_scores = 0.6745 * (df_clean[target_col] - median) / mad\n",
    "    \n",
    "    # Method 2: IQR with adaptive bounds\n",
    "    Q1 = df_clean[target_col].quantile(0.1)\n",
    "    Q3 = df_clean[target_col].quantile(0.9)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 2.0 * IQR\n",
    "    upper_bound = Q3 + 2.0 * IQR\n",
    "    \n",
    "    # Method 3: Domain-specific bounds (fuel efficiency realistic range)\n",
    "    domain_lower = 5.0  # Minimum realistic fuel efficiency\n",
    "    domain_upper = 35.0  # Maximum realistic fuel efficiency\n",
    "    \n",
    "    # Combine all methods\n",
    "    mask = (\n",
    "        (np.abs(modified_z_scores) < 3.5) &\n",
    "        (df_clean[target_col] >= lower_bound) & \n",
    "        (df_clean[target_col] <= upper_bound) &\n",
    "        (df_clean[target_col] >= domain_lower) & \n",
    "        (df_clean[target_col] <= domain_upper)\n",
    "    )\n",
    "    \n",
    "    removed_count = initial_count - mask.sum()\n",
    "    print(f\"Removed {removed_count} outliers ({removed_count/initial_count*100:.2f}%)\")\n",
    "    \n",
    "    return df_clean[mask]\n",
    "\n",
    "class SuperiorEnsemble:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.weights = {}\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = RobustScaler()\n",
    "        self.feature_selector = SelectKBest(f_regression, k='all')\n",
    "        self.feature_names = []\n",
    "        \n",
    "    def prepare_features(self, df, is_training=True):\n",
    "        \"\"\"Enhanced feature preparation with selection\"\"\"\n",
    "        df_prep = df.copy()\n",
    "        \n",
    "        # Categorical encoding\n",
    "        categorical_features = ['fuel_category', 'car_brand', 'transmission_type', 'exterior_color']\n",
    "        \n",
    "        for col in categorical_features:\n",
    "            if is_training:\n",
    "                le = LabelEncoder()\n",
    "                df_prep[col + '_encoded'] = le.fit_transform(df_prep[col].astype(str))\n",
    "                self.label_encoders[col] = le\n",
    "            else:\n",
    "                le = self.label_encoders[col]\n",
    "                df_prep[col + '_encoded'] = df_prep[col].astype(str).map(\n",
    "                    lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "                )\n",
    "        \n",
    "        # Feature selection\n",
    "        feature_cols = [\n",
    "            'engine_capacity_cc', 'owner_count', 'engine_per_owner', 'power_to_weight_ratio',\n",
    "            'fuel_efficiency_factor', 'transmission_efficiency', 'brand_efficiency_score',\n",
    "            'age_efficiency_loss', 'engine_category', 'combined_efficiency',\n",
    "            'engine_load_factor', 'displacement_per_owner', 'is_luxury', 'is_economy',\n",
    "            'color_efficiency', 'fuel_engine_interaction', 'brand_age_interaction',\n",
    "            'transmission_engine_interaction', 'engine_squared', 'owner_squared',\n",
    "            'efficiency_per_cc', 'brand_fuel_synergy'\n",
    "        ] + [col + '_encoded' for col in categorical_features]\n",
    "        \n",
    "        X = df_prep[feature_cols].fillna(0)\n",
    "        self.feature_names = feature_cols\n",
    "        \n",
    "        # Scaling\n",
    "        if is_training:\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def train_superior_models(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train optimized ensemble with additional models\"\"\"\n",
    "        \n",
    "        # Model 1: Enhanced XGBoost\n",
    "        self.models['xgb'] = xgb.XGBRegressor(\n",
    "            n_estimators=1500,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.02,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Model 2: Enhanced LightGBM\n",
    "        self.models['lgb'] = lgb.LGBMRegressor(\n",
    "            n_estimators=1500,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.02,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.1,\n",
    "            reg_lambda=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=-1\n",
    "        )\n",
    "        \n",
    "        # Model 3: Enhanced CatBoost\n",
    "        self.models['catboost'] = CatBoostRegressor(\n",
    "            iterations=1500,\n",
    "            depth=8,\n",
    "            learning_rate=0.02,\n",
    "            reg_lambda=0.1,\n",
    "            random_seed=42,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Model 4: Enhanced Random Forest\n",
    "        self.models['rf'] = RandomForestRegressor(\n",
    "            n_estimators=800,\n",
    "            max_depth=20,\n",
    "            min_samples_split=3,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='sqrt',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Model 5: Extra Trees (additional diversity)\n",
    "        self.models['et'] = ExtraTreesRegressor(\n",
    "            n_estimators=800,\n",
    "            max_depth=20,\n",
    "            min_samples_split=3,\n",
    "            min_samples_leaf=1,\n",
    "            max_features='sqrt',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Model 6: Enhanced Gradient Boosting\n",
    "        self.models['gb'] = GradientBoostingRegressor(\n",
    "            n_estimators=800,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.02,\n",
    "            subsample=0.8,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Model 7: Ridge with optimal alpha\n",
    "        self.models['ridge'] = Ridge(alpha=1.5, random_state=42)\n",
    "        \n",
    "        # Model 8: ElasticNet for additional regularization\n",
    "        self.models['elastic'] = ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=42)\n",
    "        \n",
    "        # Train all models\n",
    "        val_scores = {}\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            print(f\"Training {name}...\")\n",
    "            model.fit(X_train, y_train)\n",
    "            val_pred = model.predict(X_val)\n",
    "            val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "            val_scores[name] = val_rmse\n",
    "            print(f\"{name} - Validation RMSE: {val_rmse:.4f}\")\n",
    "        \n",
    "        # Calculate dynamic weights with temperature scaling\n",
    "        inv_scores = np.array([1/score for score in val_scores.values()])\n",
    "        temperature = 3.0  # Higher temperature for more balanced weights\n",
    "        softmax_weights = np.exp(inv_scores * temperature) / np.sum(np.exp(inv_scores * temperature))\n",
    "        \n",
    "        self.weights = dict(zip(val_scores.keys(), softmax_weights))\n",
    "        \n",
    "        print(\"\\nOptimal model weights:\")\n",
    "        for name, weight in self.weights.items():\n",
    "            print(f\"{name}: {weight:.4f}\")\n",
    "        \n",
    "        return val_scores\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make weighted ensemble predictions\"\"\"\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            pred = model.predict(X)\n",
    "            predictions += self.weights[name] * pred\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Main execution\n",
    "print(\"Starting Superior Fuel Efficiency Prediction...\")\n",
    "\n",
    "# Feature engineering\n",
    "train_enhanced = advanced_feature_engineering(train_df, is_training=True)\n",
    "test_enhanced = advanced_feature_engineering(test_df, is_training=False)\n",
    "\n",
    "# Remove outliers\n",
    "train_clean = remove_outliers_comprehensive(train_enhanced)\n",
    "print(f\"Training data shape after cleaning: {train_clean.shape}\")\n",
    "\n",
    "# Initialize ensemble\n",
    "ensemble = SuperiorEnsemble()\n",
    "\n",
    "# Prepare features\n",
    "X_full = ensemble.prepare_features(train_clean, is_training=True)\n",
    "y_full = train_clean['fuel_efficiency_kmpl'].values\n",
    "\n",
    "# Stratified split for better validation\n",
    "bins = pd.cut(y_full, bins=10, labels=False)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42, stratify=bins\n",
    ")\n",
    "\n",
    "print(\"Training Superior Ensemble...\")\n",
    "val_scores = ensemble.train_superior_models(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Ensemble validation\n",
    "ensemble_pred = ensemble.predict(X_val)\n",
    "ensemble_rmse = np.sqrt(mean_squared_error(y_val, ensemble_pred))\n",
    "ensemble_r2 = r2_score(y_val, ensemble_pred)\n",
    "ensemble_mae = mean_absolute_error(y_val, ensemble_pred)\n",
    "\n",
    "print(f\"\\nFinal Ensemble Performance:\")\n",
    "print(f\"Validation RMSE: {ensemble_rmse:.4f}\")\n",
    "print(f\"Validation R²: {ensemble_r2:.4f}\")\n",
    "print(f\"Validation MAE: {ensemble_mae:.4f}\")\n",
    "\n",
    "# Robust cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_full, bins)):\n",
    "    X_train_cv, X_val_cv = X_full[train_idx], X_full[val_idx]\n",
    "    y_train_cv, y_val_cv = y_full[train_idx], y_full[val_idx]\n",
    "    \n",
    "    temp_ensemble = SuperiorEnsemble()\n",
    "    temp_ensemble.label_encoders = ensemble.label_encoders\n",
    "    temp_ensemble.scaler = ensemble.scaler\n",
    "    temp_ensemble.train_superior_models(X_train_cv, y_train_cv, X_val_cv, y_val_cv)\n",
    "    \n",
    "    cv_pred = temp_ensemble.predict(X_val_cv)\n",
    "    cv_rmse = np.sqrt(mean_squared_error(y_val_cv, cv_pred))\n",
    "    cv_scores.append(cv_rmse)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} RMSE: {cv_rmse:.4f}\")\n",
    "\n",
    "print(f\"\\nCross-Validation Results:\")\n",
    "print(f\"Mean RMSE: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores) * 2:.4f})\")\n",
    "\n",
    "# Final predictions\n",
    "print(\"Generating final predictions...\")\n",
    "final_ensemble = SuperiorEnsemble()\n",
    "X_full_final = final_ensemble.prepare_features(train_clean, is_training=True)\n",
    "y_full_final = train_clean['fuel_efficiency_kmpl'].values\n",
    "\n",
    "# Final training\n",
    "bins_final = pd.cut(y_full_final, bins=10, labels=False)\n",
    "X_train_final, X_val_final, y_train_final, y_val_final = train_test_split(\n",
    "    X_full_final, y_full_final, test_size=0.15, random_state=42, stratify=bins_final\n",
    ")\n",
    "\n",
    "final_ensemble.train_superior_models(X_train_final, y_train_final, X_val_final, y_val_final)\n",
    "\n",
    "# Test predictions\n",
    "X_test_final = final_ensemble.prepare_features(test_enhanced, is_training=False)\n",
    "test_predictions = final_ensemble.predict(X_test_final)\n",
    "\n",
    "# Post-processing: Ensure realistic bounds\n",
    "test_predictions = np.clip(test_predictions, 8.0, 30.0)\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame({\n",
    "    'fuel_efficiency_kmpl': test_predictions\n",
    "})\n",
    "\n",
    "print(f\"\\nFinal Test Predictions Summary:\")\n",
    "print(f\"Min: {test_predictions.min():.2f}\")\n",
    "print(f\"Max: {test_predictions.max():.2f}\")\n",
    "print(f\"Mean: {test_predictions.mean():.2f}\")\n",
    "print(f\"Std: {test_predictions.std():.2f}\")\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('superior_submission.csv', index=False)\n",
    "print(\"\\nSuperior submission saved as 'superior_submission.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
